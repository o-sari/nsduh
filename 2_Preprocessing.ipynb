{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "In this notebook the data is going to be:\n",
    "* feature size reduced\n",
    "* test-train split\n",
    "* train set balanced\n",
    "* missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osari/dev/venvs/ipyth/lib64/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (2515) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57146, 2682)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/NSDUH_2015_RFD_Tab.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Remove Features\n",
    "Only keep the roughly 400 pre-screened features and the computed RFD scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array(pd.read_excel('data/clean_vars.xlsx')['vars'])\n",
    "columns = np.append(columns,(\"HERRFD\",\"TOTRFD\"))\n",
    "df1 = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57146, 401)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Train-Test Split\n",
    "\n",
    "Simple stratified 80/20 split, use total score for stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.iloc[:,0:399]\n",
    "y = df1.iloc[:,399:401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use binning for stratification\n",
    "bins     = np.linspace(0, 1, 11)\n",
    "y_binned = np.digitize(y.iloc[:,1], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 67689\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify=y_binned, \n",
    "                                                    test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11430, 401)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Balance Training Data\n",
    "\n",
    "For better training results. \n",
    "We need to balance before imputing (because imputation is expensive), but balancing doesnt support NaNs - so temporary recode them as '77777'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing:\n",
      "RFD = 0: 42882\n",
      "RFD > 0: 2834\n"
     ]
    }
   ],
   "source": [
    "print(\"Before balancing:\")\n",
    "print(\"RFD = 0:\", y_train[y_train.iloc[:,1] == 0].count()[1])\n",
    "print(\"RFD > 0:\", y_train[y_train.iloc[:,1] > 0].count()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osari/dev/venvs/ipyth/lib64/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(77777);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binned2 = (y_train.iloc[:,1] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance between TOTRFD == 0 and TOTRFD > 0\n",
    "Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "nm1 = NearMiss(version=1)\n",
    "Xy_resampled_nm1, y_resampled_nm1 = nm1.fit_resample(Xy_train, y_train_binned2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5668, 401)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_bal = pd.DataFrame(data=Xy_resampled_nm1, columns=df1.columns)\n",
    "y_train_bal = pd.DataFrame(data=Xy_resampled_nm1[:,399:401], columns=df1.columns[399:401])\n",
    "df_train_bal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing:\n",
      "RFD = 0: 2834\n",
      "RFD > 0: 2834\n"
     ]
    }
   ],
   "source": [
    "print(\"After balancing:\")\n",
    "print(\"RFD = 0:\", y_train_bal[y_train_bal.iloc[:,1] == 0].count()[1])\n",
    "print(\"RFD > 0:\", y_train_bal[y_train_bal.iloc[:,1] > 0].count()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing Values\n",
    "\n",
    "We have an amount of codes that bear no information for us, and also our NaNs (77777), first we set everything to NaN again, then we let the multivariate imputation do its job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setToNaN(df_In):\n",
    "    df_Out = df_In\n",
    "    # Replace logically assigned\n",
    "    df_Out = df_Out.replace([81, 981, 9981, 99981], 91)\n",
    "    df_Out = df_Out.replace([83, 983, 9983, 99983], 93)\n",
    "    df_Out = df_Out.replace([89, 989, 9989, 99989], 99)\n",
    "    \n",
    "    # Set everything not of interest to NaN\n",
    "    dropvals = [77777, 85, 985, 9985, 94, 994, 9994, 97, 997, 99997, 98, 998, 9998, \".\"]\n",
    "    df_Out = df_Out.replace(dropvals, np.nan)\n",
    "    \n",
    "    return df_Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace codes without content with nan to prepare imputer\n",
    "df_train_imp = setToNaN(df_train_bal)\n",
    "df_test_imp = setToNaN(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation for train (k=5668)\n",
    "df_train_imp = imp.fit_transform(df_train_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation for test (k=11430)\n",
    "df_test_imp = imp.fit_transform(df_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting np array to dataframe\n",
    "df_train_imp0 = pd.DataFrame(data=df_train_imp, columns=df1.columns)\n",
    "df_test_imp0  = pd.DataFrame(data=df_test_imp, columns=df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5668, 401)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_imp0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_imp0.to_csv(\"data/train_data.tsv.gz\", sep=\"\\t\", compression=\"gzip\")\n",
    "df_test_imp0.to_csv(\"data/test_data.tsv.gz\", sep=\"\\t\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ONE HOT \n",
    "\n",
    "took top 40 features. but we got this list after running nb3 so not sure if this code should be here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get categorical features\n",
    "df_features40 = pd.read_csv('data/feature_importance_results5.csv',sep='\\t')[:40]\n",
    "categorical_features = []\n",
    "for i in range(len(df_features40['Categorical'])):\n",
    "    if df_features40['Categorical'][i] != 0:\n",
    "        categorical_features.append(df_features40['Unnamed: 0'][i])\n",
    "len(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that are not in our 40 list\n",
    "rm = []\n",
    "for i in range(len(df_train_imp0.columns)):\n",
    "    if df_train_imp0.columns[i] not in features40:\n",
    "        rm.append(df_train_imp0.columns[i])\n",
    "df_train_imp1 = df_train_imp0.drop(columns=rm)\n",
    "df_test_imp1 = df_test_imp0.drop(columns=rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies\n",
    "df_train_encoded = pd.get_dummies(df_train_imp1, columns=categorical_features)\n",
    "df_test_encoded = pd.get_dummies(df_test_imp1, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "df_test_encoded.to_csv('data/test_encoded.tsv',sep='\\t',index=False,header=False)\n",
    "df_train_encoded.to_csv('data/train_encoded.tsv',sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
